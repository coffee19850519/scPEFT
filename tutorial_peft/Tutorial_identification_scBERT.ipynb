{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PEFT(Parameter-Efficient Fine-Tuning) on Pre-trained Model(scBERT) for Cell-type Identification",
   "id": "917f174a0d0f216e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this tutorial, we demonstrate how to peft(Parameter-Efficient Fine-Tuning) a pre-trained (scBERT) model on a new dataset for the cell type identification task. We use the Multiple Sclerosis dataset as an example and peft on the pre-trained whole-body model.",
   "id": "4606cc45ffc3c552"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "please download the scBERT pretrained files and checkpoints [panglao_human.h5ad](https://mailmissouri-my.sharepoint.com/:u:/g/personal/hefe_umsystem_edu/EUDkCqqnk2hOiaOl5FSxn5gBQCIbmBDlLDvOmsP41doFWw?e=HZ4ck3) and [panglao_pretrain.pth](https://mailmissouri-my.sharepoint.com/:u:/g/personal/hefe_umsystem_edu/EVesK-hwXoJGq4KNeZ0bewoBiGRjEsdHInv801GL8zBonw?e=6F339j) into pipeline_scBERT path, and download the scBERT gene tokenizer parameter file [gene2vec_16906.npy](https://mailmissouri-my.sharepoint.com/:u:/g/personal/hefe_umsystem_edu/EVuLpRYVokpKgfppwFG6inEB8IN05BLX5OBmht3v6eir2g?e=bV9fi4) and put it into the performer_pytorch path.",
   "id": "d3d76ce096f5282"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 1: Modify the parameters\n",
    "There are four key settings that the reader needs to modify. The available options are listed below:\n",
    "\n",
    "data_name : NSCLC/COVID/ms_scBERT,\n",
    "\n",
    "data_path: {data_path}/celltype_annotation,\n",
    "\n",
    "model_path: {checkpoint_path},\n",
    "\n",
    "prompt_type: Gene_encoder_prompt/ Gene_token_prompt / prefix_prompt / LoRA,\n",
    "\n",
    "pretrain_data_path:{pretrain_data_path}/panglao_human.h5ad."
   ],
   "id": "83569dc2815349c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--data_name\", type=str, default='COVID',help='NSCLC/COVID/ms')\n",
    "parser.add_argument(\"--data_path\", type=str, default='../data/celltype_identification/', help='Path of data for PEFT.')\n",
    "parser.add_argument(\"--prompt_type\", type=str, default='Gene_token_prompt',help='Gene_encoder_prompt/Gene_token_prompt/prefix_prompt /LoRA')\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 2: import dependencies",
   "id": "157c42ddf933b5f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import argparse\n",
    "import copy\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from pipeline_scBERT.performer_pytorch.performer_pytorch_prompt import PerformerLM\n",
    "import scanpy as sc\n",
    "from pipeline_scBERT.utils import *\n",
    "from pipeline_scBERT.Benckmark_utils import scBERT_preprocess\n",
    "import warnings\n",
    "from sklearn.metrics import f1_score, balanced_accuracy_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "ee02c5fc49cdb1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "parser.add_argument(\"--model_path\", type=str, default='../scBERT_ckpts', help='Path of pretrained model.')\n",
    "parser.add_argument(\"--pretrain_data_path\", type=str, default='../pipeline_scBERT/panglao_human.h5ad', help='Path of pretrain data.')\n",
    "parser.add_argument(\"--model_path\", type=str, default='../scBERT_ckpts', help='Path of pretrained model.')\n",
    "parser.add_argument(\"--batch_size\", type=int, default=1, help='Number of batch size.')\n",
    "parser.add_argument(\"--pretrain_data_path\", type=str, default='./panglao_human.h5ad', help='Path of pretrain data.')\n",
    "parser.add_argument(\"--bin_num\", type=int, default=5, help='Number of bins.')\n",
    "parser.add_argument(\"--gene_num\", type=int, default=16906, help='Number of genes.')  # 2000\n",
    "parser.add_argument(\"--seed\", type=int, default=2021, help='Random seed.')\n",
    "parser.add_argument(\"--pos_embed\", type=bool, default=True, help='Using Gene2vec encoding or not.')\n",
    "parser.add_argument(\"--tokens\", type=str, default=64, help='prefix token number')\n",
    "parser.add_argument(\"--space_conf\", type=str, default=[1, 1, 1, 1, 1, 1],\n",
    "                    help='encoder space adapter list')  # [0，0，0, 1, 1, 1]\n",
    "parser.add_argument(\"--mlp_conf\", type=str, default=[0, 0, 0, 0, 0, 0], help='encoder mlp adapter list')\n",
    "parser.add_argument(\"--mlp_ratio\", type=float, default=0.25, help='mlp_ratio.')\n",
    "args = parser.parse_args()"
   ],
   "id": "5fc140553f837e82"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 3:start training",
   "id": "ecbdb969abae4d46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt_type = args.prompt_type\n",
    "SEED = args.seed\n",
    "BATCH_SIZE = args.batch_size\n",
    "SEQ_LEN = args.gene_num + 1\n",
    "\n",
    "UNASSIGN_THRES = 0.0\n",
    "CLASS = args.bin_num + 2\n",
    "POS_EMBED_USING = args.pos_embed\n",
    "data_name = args.data_name\n",
    "data_path = args.data_path\n",
    "pretrain_data = args.pretrain_data_path\n",
    "device = torch.device(f\"cuda:0\")\n",
    "tokens = args.tokens\n",
    "space_conf = args.space_conf\n",
    "mlp_conf = args.mlp_conf\n",
    "mlp_ratio = args.mlp_ratio\n"
   ],
   "id": "c96a547f2f2cb8f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "class Identity(torch.nn.Module):\n",
    "    def __init__(self, dropout=0., h_dim=100, out_dim=10):\n",
    "        super(Identity, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 1, (1, 200))\n",
    "        self.act = nn.ReLU()\n",
    "        if prompt_type == \"prefix_prompt\":\n",
    "            self.fc1 = nn.Linear(in_features=SEQ_LEN + 64, out_features=512, bias=True)\n",
    "            # self.fc1 = nn.Linear(in_features=sum(gene_mask)+1 + 64, out_features=512, bias=True)\n",
    "\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(in_features=SEQ_LEN, out_features=512, bias=True)\n",
    "            # self.fc1 = nn.Linear(in_features=sum(gene_mask)+1, out_features=512, bias=True)\n",
    "\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(in_features=512, out_features=h_dim, bias=True)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.fc3 = nn.Linear(in_features=h_dim, out_features=out_dim, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:, None, :, :]\n",
    "        x = self.conv1(x)\n",
    "        x = self.act(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        # print(x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.act2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ],
   "id": "c798d36d15c5936a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for fold_idx in range(5):\n",
    "\n",
    "    data_train = sc.read_h5ad(f'{data_path}/{data_name}/{str(fold_idx)}/{data_name}_train{str(fold_idx)}.h5ad')\n",
    "    data_val = sc.read_h5ad(f'{data_path}/{data_name}/{str(fold_idx)}/{data_name}_val{str(fold_idx)}.h5ad')\n",
    "    data_test = sc.read_h5ad(f'{data_path}/{data_name}/{str(fold_idx)}/{data_name}_test{str(fold_idx)}.h5ad')\n",
    "\n",
    "    data_train.obs[\"str_batch\"] = \"0\"\n",
    "    data_test.obs[\"str_batch\"] = \"1\"\n",
    "    data_val.obs[\"str_batch\"] = \"2\"\n",
    "    data_train = data_train.concatenate((data_test, data_val), batch_key=\"str_batch\")\n",
    "\n",
    "    if data_name == 'ms':\n",
    "        data_is_raw = False\n",
    "        celltype_key = 'celltype'\n",
    "        data_train.var_names = data_train.var[\"gene_name\"]\n",
    "    elif data_name == 'COVID':\n",
    "        data_is_raw = True\n",
    "        celltype_key = 'cell_type'\n",
    "    elif data_name == 'NSCLC':\n",
    "        data_is_raw = True\n",
    "        celltype_key = 'cell_type'\n",
    "\n",
    "    panglao = sc.read_h5ad(pretrain_data)\n",
    "    preprocess = scBERT_preprocess(panglao, data_train)\n",
    "    data_train = preprocess(data_is_raw=data_is_raw)\n",
    "    gene_mask = data_train.uns[\"gene_mask\"]\n",
    "    del panglao\n",
    "\n",
    "    label_dict, _ = np.unique(np.array(data_train.obs[celltype_key]), return_inverse=True)\n",
    "    data_test = data_train[data_train.obs[\"str_batch\"] == \"1\"]\n",
    "\n",
    "    _, label_test = np.unique(np.array(data_test.obs[celltype_key]), return_inverse=True)\n",
    "    label_test = torch.from_numpy(label_test)\n",
    "\n",
    "    data_test = data_test.X\n",
    "    test_dataset = SCDataset(data_test, label_test)\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "    model = PerformerLM(\n",
    "        num_tokens=CLASS,\n",
    "        dim=200,\n",
    "        depth=6,\n",
    "        max_seq_len=SEQ_LEN,\n",
    "        heads=10,\n",
    "        local_attn_heads=0,\n",
    "        g2v_position_emb=POS_EMBED_USING,\n",
    "        prompt_type=prompt_type,\n",
    "        tokens=tokens,\n",
    "        space_conf=space_conf,\n",
    "        mlp_conf=mlp_conf,\n",
    "        gene_mask=gene_mask,\n",
    "        mlp_ratio=mlp_ratio,\n",
    "    )\n",
    "\n",
    "    path = args.model_path\n",
    "    path = f\"{path}/{data_name}/scPEFT/{fold_idx}/best_model.pt\"\n",
    "    ckpt = torch.load(path)\n",
    "    model.to_out = Identity(dropout=0., h_dim=128, out_dim=label_dict.shape[0])\n",
    "\n",
    "    model.load_state_dict(ckpt, True)\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    def test(model: nn.Module, test_loader: DataLoader) -> float:\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        truths = []\n",
    "        for index, (data_t, labels_t) in tqdm(enumerate(test_loader), total=len(test_loader), desc=\"Inference\",\n",
    "                                              leave=True):\n",
    "            index += 1\n",
    "            data_t, labels_t = data_t.to(device), labels_t.to(device)\n",
    "            logits = model(data_t)\n",
    "            softmax = nn.Softmax(dim=-1)\n",
    "            final_prob = softmax(logits)\n",
    "            final = final_prob.argmax(dim=-1)\n",
    "            final[np.amax(np.array(final_prob.detach().cpu().numpy()), axis=-1) < UNASSIGN_THRES] = -1\n",
    "            predictions.append(final.detach().cpu().numpy())\n",
    "            truths.append(labels_t.detach().cpu().numpy())\n",
    "        predictions = np.concatenate(predictions, axis=0)\n",
    "        truths = np.concatenate(truths, axis=0)\n",
    "        return predictions, truths\n",
    "\n",
    "\n",
    "    predictions, celltypes_labels = test(model, test_loader)\n",
    "    balanced_accuracy = balanced_accuracy_score(celltypes_labels, predictions)\n",
    "    f1 = f1_score(celltypes_labels, predictions, average=\"macro\")\n",
    "\n",
    "    print(f\"fold_{fold_idx}: Accuracy: {balanced_accuracy:.3f}, \"f\"macro F1: {f1:.3f}\", flush=True)"
   ],
   "id": "a3604828665ca61d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
