{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PEFT(Parameter-Efficient Fine-Tuning) on Pre-trained Model(Geneformer) for Cell-type Identification",
   "id": "917f174a0d0f216e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this tutorial, we demonstrate how to peft(Parameter-Efficient Fine-Tuning) a pre-trained (Geneformer) model on a new dataset for the cell type identification task. We use the Multiple Sclerosis dataset as an example and peft on the pre-trained whole-body model.",
   "id": "4606cc45ffc3c552"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "please download the Geneformer pretrained files and checkpoints [geneformer-12L-30M-prompt](https://mailmissouri-my.sharepoint.com/:f:/g/personal/hefe_umsystem_edu/EnMxoKxbenxIpwvVIxThH3EB_GcH0d0gIpEW_J_3riNhSA?e=9hfu2Z) into geneformer_peft/Pretrain_ckpts path.",
   "id": "b57dc24e8757f6b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 1: Modify the parameters\n",
    "There are four key settings that the reader needs to modify. The available options are listed below:\n",
    "\n",
    "dataset_name : NSCLC/COVID/ms,\n",
    "\n",
    "data_path: {data_path},\n",
    "\n",
    "output_path: {output_path},\n"
   ],
   "id": "83569dc2815349c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--dataset_name\", type=str, default='ms',help='NSCLC/COVID/ms')\n",
    "parser.add_argument(\"--data_path\", type=str, default='../data/celltype_identification/', help='Path of data for predicting')\n",
    "parser.add_argument(\"--output_path\", type=str, default=f\"../Geneformer/\", help='output data path.')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 2: import dependencies",
   "id": "157c42ddf933b5f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Geneformer Fine-Tuning for Cell Annotation Application\n",
    "import argparse\n",
    "# %%\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import datasets\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "from accelerate.utils import is_datasets_available\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from geneformer_peft.transformerslocal.src.transformers.trainer_utils import seed_worker\n",
    "\n",
    "from numpy import mean\n",
    "\n",
    "import torch\n",
    "\n",
    "from geneformer_peft.transformerslocal.src.transformers import EarlyStoppingCallback\n",
    "\n",
    "# device\n",
    "# device = torch.device(\"cuda:3\")\n",
    "# import\n",
    "import torch.nn as nn\n",
    "# imports\n",
    "from collections import Counter\n",
    "import datetime\n",
    "import pickle\n",
    "import subprocess\n",
    "import seaborn as sns;\n",
    "\n",
    "sns.set()\n",
    "from datasets import load_from_disk\n",
    "from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score, precision_score, recall_score\n",
    "from geneformer_peft.transformerslocal.src.transformers.models.bert.modeling_bert import BertForSequenceClassification\n",
    "# from transformers  import BertForSequenceClassification\n",
    "from transformers import Trainer\n",
    "from transformers.training_args import TrainingArguments\n",
    "from geneformer_peft.geneformer import DataCollatorForCellClassification\n",
    "import numpy as np\n",
    "import loralib as lora"
   ],
   "id": "ee02c5fc49cdb1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "parser.add_argument(\"--epoch\", type=int, default=25, help='Number of epochs.')\n",
    "parser.add_argument(\"--use_prompt\", type=bool, default=True, help='whether use prompt or not.')\n",
    "parser.add_argument(\"--lr\", type=int, default=0.000804, help='')\n",
    "parser.add_argument(\"--batch_size\", type=int, default=20, help='')\n",
    "args = parser.parse_args()"
   ],
   "id": "5fc140553f837e82"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 3:start training",
   "id": "ecbdb969abae4d46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "dataset_name = args.dataset_name\n",
    "\n",
    "data_dir = args.data_path\n",
    "accuracys = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "macro_f1s = []\n",
    "mic_precisions = []\n",
    "mic_recalls = []\n",
    "micro_f1s = []\n",
    "if dataset_name == \"MergedHuman\":\n",
    "    n_splits = 4\n",
    "else:\n",
    "    n_splits = 5\n"
   ],
   "id": "c96a547f2f2cb8f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "for i in range(n_splits):\n",
    "\n",
    "    train_data_dir = f'{data_dir}/{dataset_name}/{i}/{dataset_name}_train{i}.dataset'\n",
    "    eval_data_dir = f'{data_dir}/{dataset_name}/{i}/{dataset_name}_val{i}.dataset'\n",
    "    test_data_dir = f'{data_dir}/{dataset_name}/{i}/{dataset_name}_test{i}.dataset'\n",
    "    train_dataset = load_from_disk(train_data_dir)\n",
    "    eval_dataset = load_from_disk(eval_data_dir)\n",
    "    test_dataset = load_from_disk(test_data_dir)\n",
    "\n",
    "    dataset_list = []\n",
    "    evalset_list = []\n",
    "    organ_list = []\n",
    "    testset_list = []\n",
    "    target_dict_list = []\n",
    "\n",
    "    trainset_organ = train_dataset\n",
    "    evalset_organ = eval_dataset\n",
    "    testset_organ = test_dataset\n",
    "    # per scDeepsort published method, drop cell types representing <0.5% of cells\n",
    "    celltype_counter = Counter(trainset_organ[\"cell_type\"])\n",
    "    total_cells = sum(celltype_counter.values())\n",
    "    cells_to_keep = [k for k, v in celltype_counter.items() if v > (0.005 * total_cells)]\n",
    "\n",
    "    trainset_organ_subset = trainset_organ\n",
    "    evalset_organ_subset = evalset_organ\n",
    "    testset_organ_subset = testset_organ\n",
    "\n",
    "    # shuffle datasets and rename columns\n",
    "    trainset_organ_shuffled = trainset_organ_subset.shuffle(seed=42)\n",
    "    trainset_organ_shuffled = trainset_organ_shuffled.rename_column(\"cell_type\", \"label\")\n",
    "\n",
    "    evalset_organ_shuffled = evalset_organ_subset.shuffle(seed=42)\n",
    "    evalset_organ_shuffled = evalset_organ_shuffled.rename_column(\"cell_type\", \"label\")\n",
    "\n",
    "    testset_organ_shuffled = testset_organ_subset.shuffle(seed=42)\n",
    "    testset_organ_shuffled = testset_organ_shuffled.rename_column(\"cell_type\", \"label\")\n",
    "\n",
    "    # create dictionary of cell types : label ids\n",
    "    target_names = list(Counter(\n",
    "        trainset_organ_shuffled[\"label\"] + evalset_organ_shuffled[\"label\"] + testset_organ_shuffled[\"label\"]).keys())\n",
    "    target_name_id_dict = dict(zip(target_names, [i for i in range(len(target_names))]))\n",
    "    target_dict_list += [target_name_id_dict]\n",
    "    target_name_id_celltype_dict = {value: key for key, value in target_name_id_dict.items()}\n",
    "\n",
    "\n",
    "    # change labels to numerical ids\n",
    "    def classes_to_ids(example):\n",
    "        example[\"label\"] = target_name_id_dict[example[\"label\"]]\n",
    "        return example\n",
    "\n",
    "\n",
    "    labeled_trainset = trainset_organ_shuffled.map(classes_to_ids, num_proc=16)\n",
    "    labeled_evalset = evalset_organ_shuffled.map(classes_to_ids, num_proc=16)\n",
    "    labeled_testset = testset_organ_shuffled.map(classes_to_ids, num_proc=16)\n",
    "\n",
    "    labeled_train_split = labeled_trainset\n",
    "    labeled_eval_split = labeled_evalset\n",
    "\n",
    "    labeled_test_split_subset = labeled_testset\n",
    "    trained_labels = list(Counter(labeled_train_split[\"label\"]).keys())\n",
    "    labeled_eval_split_subset = labeled_eval_split\n",
    "\n",
    "    dataset_list += [labeled_train_split]\n",
    "    evalset_list += [labeled_eval_split_subset]\n",
    "    testset_list += [labeled_test_split_subset]\n",
    "    # %%\n",
    "\n",
    "    trainset_dict = dataset_list\n",
    "    traintargetdict_dict = target_dict_list\n",
    "    evalset_dict = evalset_list\n",
    "    testset_dict = testset_list\n",
    "\n",
    "\n",
    "    def SaveProcessResult(labels, pred_cellcodes, confidence_socre):\n",
    "        d_save = {'pred_celltype': [target_name_id_celltype_dict[pred_cellcode] for pred_cellcode in pred_cellcodes],\n",
    "                  'gt_celltype': [target_name_id_celltype_dict[label] for label in labels],\n",
    "                  'pred_cellcode': pred_cellcodes,\n",
    "                  'gt_cellcode': labels,\n",
    "                  'confidence_socre': confidence_socre\n",
    "                  }\n",
    "        pd.DataFrame(data=d_save).to_csv(f'{output_dir}/Geneformer_dataset{dataset_name}_result.csv')\n",
    "\n",
    "\n",
    "    def compute_metrics(pred):\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "\n",
    "        # save confidence score and label to csv\n",
    "        confidence_score = nn.Softmax(dim=-1)(torch.from_numpy(pred.predictions)).detach().cpu().numpy().tolist()\n",
    "        labels_tocsv = pred.label_ids.tolist()\n",
    "        SaveProcessResult(labels_tocsv, preds, confidence_score)\n",
    "\n",
    "        # calculate accuracy and macro f1 using sklearn's function\n",
    "        acc = balanced_accuracy_score(labels, preds)\n",
    "        precision = precision_score(labels, preds, average='macro')\n",
    "        recall = recall_score(labels, preds, average='macro')\n",
    "        macro_f1 = f1_score(labels, preds, average='macro')\n",
    "\n",
    "        micro_precision = precision_score(labels, preds, average='micro')\n",
    "        micro_recall = recall_score(labels, preds, average='micro')\n",
    "        micro_f1 = f1_score(labels, preds, average='micro')\n",
    "        return {\n",
    "            'accuracy': acc,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'macro_f1': macro_f1,\n",
    "            \"micro_precision\": micro_precision,\n",
    "            \"micro_recall\": micro_recall,\n",
    "            \"micro_f1\": micro_f1\n",
    "\n",
    "        }\n",
    "\n",
    "\n",
    "    # set model parameters\n",
    "    # max input size\n",
    "    max_input_size = 2 ** 11  # 2048\n",
    "    max_lr = args.lr\n",
    "    # how many pretrained layers to freeze\n",
    "    freeze_layers = 0\n",
    "    # number gpus\n",
    "    num_gpus = 1\n",
    "    # number cpu cores\n",
    "    num_proc = 16\n",
    "    # batch size for training and eval\n",
    "    geneformer_batch_size = args.batch_size\n",
    "    # learning schedule\n",
    "    lr_schedule_fn = \"linear\"\n",
    "    # warmup steps\n",
    "    warmup_steps = 500\n",
    "    # number of epochs\n",
    "    epochs = args.epoch\n",
    "    # optimizer\n",
    "    optimizer = \"adamw\"\n",
    "\n",
    "    # %%\n",
    "    celltype_id_labels = [target_name_id_dict[item] for item in train_dataset[\"cell_type\"]]\n",
    "    class_num = np.unique(celltype_id_labels, return_counts=True)[1].tolist()\n",
    "    class_weight = torch.tensor([(1 - (x / sum(class_num))) ** 2 for x in class_num])\n",
    "    weighted_loss = nn.CrossEntropyLoss(weight=class_weight)\n",
    "\n",
    "    organ_trainset = trainset_dict[0]\n",
    "    organ_evalset = evalset_dict[0]\n",
    "    organ_testset = testset_dict[0]\n",
    "    organ_label_dict = traintargetdict_dict[0]\n",
    "    logging_steps = 1\n",
    "\n",
    "    # reload pretrained model\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        \"geneformer_peft/Pretrain_ckpts/geneformer-12L-30M-prompt\",\n",
    "        num_labels=len(organ_label_dict.keys()),\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        ignore_mismatched_sizes=True).to(\"cuda\")\n",
    "    prompt_type = model.config.prompt_type\n",
    "    output_dir = args.output_path + prompt_type + '/' + dataset_name + f\"/{i}\"\n",
    "    model.config.save_pretrained(output_dir)\n",
    "    print(output_dir)\n",
    "    pre_freeze_param_count = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters()).values())\n",
    "\n",
    "    keywords = [\n",
    "        'lora_A',\n",
    "        'lora_B',\n",
    "        'lora_value.bias',\n",
    "        'lora_key.bias',\n",
    "        'lora_query.bias',\n",
    "        'prompt_embeddings',\n",
    "        'adapter',\n",
    "        'Adapter'\n",
    "    ]\n",
    "\n",
    "    for name, para in model.named_parameters():\n",
    "        para.requires_grad = False\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "    for param in model.bert.pooler.parameters():\n",
    "        param.requires_grad = True\n",
    "    for name, para in model.named_parameters():\n",
    "        if any(keyword in name for keyword in keywords):\n",
    "            para.requires_grad = True\n",
    "\n",
    "    print(\"-\" * 89)\n",
    "    learnable_params = {k: v for k, v in model.named_parameters() if v.requires_grad == True}\n",
    "    for k, v in learnable_params.items():\n",
    "        print(f\"Learnable params {k} with shape {v.shape}\")\n",
    "\n",
    "    post_freeze_param_count = sum(\n",
    "        dict((p.data_ptr(), p.numel()) for p in model.parameters() if p.requires_grad).values())\n",
    "    print(\"Total Pre freeze Params: %.2fM\" % (pre_freeze_param_count / 1e6,))\n",
    "    print(\"Total Post freeze Params: %.2fM\" % (post_freeze_param_count / 1e6,))\n",
    "\n",
    "\n",
    "    class MyTrainer(Trainer):\n",
    "        def compute_loss(self, model, inputs, return_outputs=False):\n",
    "            if model.config.prompt_type == \"prefix_prompt\":\n",
    "                mask_cat = torch.ones([inputs[\"input_ids\"].shape[0], model.config.num_token]).to(\"cuda\")\n",
    "                mask_middle = torch.cat((inputs[\"attention_mask\"][:, :1], mask_cat), dim=1)\n",
    "                inputs[\"attention_mask\"] = torch.cat((mask_middle, inputs[\"attention_mask\"][:, 1:]), dim=1)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss_fct = weighted_loss.to(outputs['logits'].device)\n",
    "            loss = loss_fct(outputs['logits'], inputs['labels'])\n",
    "\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "        def get_train_dataloader(self) -> DataLoader:\n",
    "            \"\"\"\n",
    "            Returns the training [`~torch.utils.data.DataLoader`].\n",
    "\n",
    "            Will use no sampler if `train_dataset` does not implement `__len__`, a random sampler (adapted to distributed\n",
    "            training if necessary) otherwise.\n",
    "\n",
    "            Subclass and override this method if you want to inject some custom behavior.\n",
    "            \"\"\"\n",
    "            if self.train_dataset is None:\n",
    "                raise ValueError(\"Trainer: training requires a train_dataset.\")\n",
    "\n",
    "            train_dataset = self.train_dataset\n",
    "            data_collator = self.data_collator\n",
    "            if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\n",
    "                train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\n",
    "            else:\n",
    "                data_collator = self._get_collator_with_removed_columns(data_collator, description=\"training\")\n",
    "\n",
    "            dataloader_params = {\n",
    "                \"batch_size\": self._train_batch_size,\n",
    "                \"collate_fn\": data_collator,\n",
    "                \"num_workers\": self.args.dataloader_num_workers,\n",
    "                \"pin_memory\": self.args.dataloader_pin_memory,\n",
    "            }\n",
    "\n",
    "            if not isinstance(train_dataset, torch.utils.data.IterableDataset):\n",
    "                train_class_num = np.unique(train_dataset[\"label\"], return_counts=True)[1]\n",
    "                sample_weights = 1.0 / train_class_num[train_dataset[\"label\"]]\n",
    "                sample_weights = sample_weights / np.sum(sample_weights)\n",
    "                train_num = train_dataset.shape[0]\n",
    "                train_sampler = torch.utils.data.sampler.WeightedRandomSampler(sample_weights, train_num,\n",
    "                                                                               replacement=True)\n",
    "                dataloader_params[\"sampler\"] = train_sampler\n",
    "                dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n",
    "                dataloader_params[\"worker_init_fn\"] = seed_worker\n",
    "\n",
    "            return self.accelerator.prepare(DataLoader(train_dataset, **dataloader_params))\n",
    "\n",
    "\n",
    "    # define output directory path\n",
    "    current_date = datetime.datetime.now()\n",
    "    datestamp = f\"{str(current_date.year)[-2:]}-{current_date.month:02d}-{current_date.day:02d}_{current_date.hour:02d}:{current_date.minute:02d}:{current_date.second:02d}\"\n",
    "\n",
    "    # ensure not overwriting previously saved model\n",
    "    saved_model_test = os.path.join(output_dir, f\"pytorch_model.bin\")\n",
    "    model.config.save_pretrained(output_dir)\n",
    "    # print(datestamp)\n",
    "    if os.path.isfile(saved_model_test) == True:\n",
    "        raise Exception(\"Model already saved to this directory.\")\n",
    "\n",
    "    # make output directory\n",
    "    subprocess.call(f'mkdir {output_dir}', shell=True)\n",
    "    # set training arguments\n",
    "    training_args = {\n",
    "        \"learning_rate\": max_lr,\n",
    "        \"do_train\": True,\n",
    "        \"do_eval\": True,\n",
    "        \"evaluation_strategy\": \"epoch\",\n",
    "        \"save_strategy\": \"epoch\",\n",
    "        \"logging_steps\": logging_steps,\n",
    "        \"group_by_length\": True,\n",
    "        \"length_column_name\": \"length\",\n",
    "        \"disable_tqdm\": False,\n",
    "        \"lr_scheduler_type\": lr_schedule_fn,\n",
    "        \"warmup_steps\": warmup_steps,\n",
    "        \"weight_decay\": 0.001,\n",
    "        \"per_device_train_batch_size\": geneformer_batch_size,\n",
    "        \"per_device_eval_batch_size\": geneformer_batch_size,\n",
    "        \"num_train_epochs\": epochs,\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"output_dir\": output_dir,\n",
    "        \"save_safetensors\": False,\n",
    "    }\n",
    "\n",
    "    training_args_init = TrainingArguments(**training_args)\n",
    "\n",
    "    # create the trainer\n",
    "    trainer = MyTrainer(\n",
    "        model=model,\n",
    "        args=training_args_init,\n",
    "        data_collator=DataCollatorForCellClassification(),\n",
    "        train_dataset=organ_trainset,\n",
    "        eval_dataset=organ_evalset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "    )\n",
    "    # train the cell type classifier\n",
    "    trainer.train()\n",
    "    predictions = trainer.predict(organ_testset)\n",
    "\n",
    "    print(f\"fold{i}:\"\n",
    "          f\"macro Accuracy: {predictions.metrics['test_accuracy']:.3f}, macro Precision: {predictions.metrics['test_precision']:.3f},macro Recall: {predictions.metrics['test_recall']:.3f}, \"\n",
    "          f\"macro F1: {predictions.metrics['test_macro_f1']:.3f}\"\n",
    "          )\n",
    "    print(f\"fold{i}:\"\n",
    "          f\"micro Accuracy: {predictions.metrics['test_accuracy']:.3f}, micro Precision: {predictions.metrics['test_micro_precision']:.3f}, micro Recall: {predictions.metrics['test_micro_recall']:.3f}, \"\n",
    "          f\"micro F1: {predictions.metrics['test_micro_f1']:.3f}\")"
   ],
   "id": "c798d36d15c5936a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
