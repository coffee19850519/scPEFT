{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# PEFT(Parameter-Efficient Fine-Tuning) on Pre-trained Model(scFoundation) for Cell-type Identification",
   "id": "917f174a0d0f216e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this tutorial, we demonstrate how to peft(Parameter-Efficient Fine-Tuning) a pre-trained (scFoundation) model on a new dataset for the cell type identification task. We use the Multiple Sclerosis dataset as an example and peft on the pre-trained whole-body model.",
   "id": "4606cc45ffc3c552"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "please download the scFoundation [pretrained files and checkpoints](https://mailmissouri-my.sharepoint.com/:f:/g/personal/hefe_umsystem_edu/EoCITFRQ1AlCs-fVtoSD_oABgxJU7TGHIyx84kRq_3mw2w?e=wbsd9l) into scFoundation/annotations/models",
   "id": "d3d76ce096f5282"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 1: Modify the parameters\n",
    "There are four key settings that the reader needs to modify. The available options are listed below:\n",
    "\n",
    "data_name : NSCLC/COVID/ms_scBERT,\n",
    "\n",
    "data_path: {data_path},\n",
    "\n",
    "peft_type: Gene_encoder_prompt/ Gene_token_prompt / prefix_prompt / LoRA,\n",
    "\n",
    "save_pathï¼šPath to output"
   ],
   "id": "83569dc2815349c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--data_name\", type=str, default='ms',\n",
    "                    help='NSCLC/COVID/ms')\n",
    "parser.add_argument(\"--data_path\", type=str, default='./data',\n",
    "                    help='Path of data for PEFT.')\n",
    "parser.add_argument(\"--peft_type\", type=str, default='Gene_token_prompt',help='Gene_encoder_prompt/Gene_token_prompt/prefix_prompt /LoRA')\n",
    "parser.add_argument(\"--save_path\", type=str, default=f\"/cluster/pixstor/xudong-lab/yangyu/Geneformer_unsupervise/cross_species/\",\n",
    "                    help='Path of data for predicting.')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 2: import dependencies",
   "id": "157c42ddf933b5f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import argparse\n",
    "import random,os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import scipy.sparse\n",
    "from scipy.sparse import issparse\n",
    "import scanpy as sc\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from scfoundation.annotation.load import *\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import copy\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, precision_recall_fscore_support, \\\n",
    "    classification_report, balanced_accuracy_score\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "ee02c5fc49cdb1e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "parser.add_argument('--task_name', type=str, default='deepcdr', help='task name')\n",
    "parser.add_argument('--input_type', type=str, default='singlecell',choices=['singlecell','bulk'], help='input type; default: singlecell')\n",
    "parser.add_argument('--output_type', type=str, default='gene',choices=['cell','gene','gene_batch','gene_expression'], help='cell or gene embedding; default: cell the difference between gene and gene_batch is that in gene mode the gene embedding will be processed one by one. while in gene_batch mode, the gene embedding will be processed in batch. GEARS use gene_batch mode.')\n",
    "parser.add_argument('--pool_type', type=str, default='max',choices=['all','max'], help='pooling type of cell embedding; default: all only valid for output_type=cell')\n",
    "parser.add_argument('--tgthighres', type=str, default='t4', help='the targeted high resolution (start with t) or the fold change of the high resolution (start with f), or the addtion (start with a) of the high resoultion. only valid for input_type=singlecell')\n",
    "parser.add_argument('--pre_normalized', type=str, default='F',choices=['F','T','A'], help='if normalized before input; default: False (F). choice: True(T), Append(A) When input_type=bulk: pre_normalized=T means log10(sum of gene expression). pre_normalized=F means sum of gene expression without normalization. When input_type=singlecell: pre_normalized=T or F means gene expression is already normalized+log1p or not. pre_normalized=A means gene expression is normalized and log1p transformed. the total count is appended to the end of the gene expression matrix.')\n",
    "parser.add_argument('--demo', action='store_true', default=False, help='if demo, only infer 10 samples')\n",
    "parser.add_argument('--version',  type=str, default='ce', help='only valid for output_type=cell. For read depth enhancemnet, version=rde For others, version=ce')\n",
    "parser.add_argument('--model_path',  type=str, default='None', help='pre-trained model path')\n",
    "parser.add_argument('--ckpt_name',  type=str, default='01B-resolution', help='checkpoint name')\n",
    "parser.add_argument(\"--use_prompt\", type=bool, default=True)\n",
    "parser.add_argument(\"--fold_idx\", type=str, default='0')\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help='Number of batch size.')\n",
    "parser.add_argument(\"--epoch\", type=int, default=100, help='Number of epochs.')\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=1e-5, help='Learning rate.')\n",
    "\n",
    "args = parser.parse_args()"
   ],
   "id": "5fc140553f837e82"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Step 3:start training",
   "id": "ecbdb969abae4d46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n_layers_conf=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  # token\n",
    "mlp_adapter_conf=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "space_adapter_conf=[1, 1, 1, 1, 1, 1,1, 1, 1, 1, 1, 1]\n",
    "# mlp_adapter_conf=[0,0,0,0,0,0, 1, 1, 1, 1, 1, 1]\n",
    "# space_adapter_conf=[0,0,0,0,0,0, 1, 1, 1, 1, 1, 1]\n",
    "peft_prompt_relationship = {\n",
    "    \"Encoder_adapter\": \"encoder-prompt\",\n",
    "    \"Token_adapter\": \"head-prompt\",\n",
    "    \"Prefix\": \"prefix-prompt\",\n",
    "    \"LoRA\": \"LoRA\",\n",
    "    \"finetune\": \"finetune\"\n",
    "}\n",
    "\n",
    "prompt_type = peft_prompt_relationship[args.peft_type]\n",
    "prompt_settings = {\n",
    "    \"use_prompt\": args.use_prompt,\n",
    "    \"num_tokens\": 64,\n",
    "    \"prompt_type\": prompt_type,\n",
    "    \"n_layers_conf\": n_layers_conf,\n",
    "    \"mlp_adapter_conf\": mlp_adapter_conf,\n",
    "    \"space_adapter_conf\": space_adapter_conf\n",
    "}\n",
    "\n",
    "if args.data_name == 'ms':\n",
    "    args.pre_normalized = 'T'\n",
    "    print(\"pre normalized\",args.pre_normalized)\n",
    "\n",
    "ckpt_dir = args.save_path\n",
    "LEARNING_RATE = args.learning_rate\n"
   ],
   "id": "c96a547f2f2cb8f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "def main_gene_selection(X_df, gene_list):\n",
    "    \"\"\"\n",
    "    Describe:\n",
    "        rebuild the input adata to select target genes encode protein\n",
    "    Parameters:\n",
    "        adata->`~anndata.AnnData` object: adata with var index_name by gene symbol\n",
    "        gene_list->list: wanted target gene\n",
    "    Returns:\n",
    "        adata_new->`~anndata.AnnData` object\n",
    "        to_fill_columns->list: zero padding gene\n",
    "    \"\"\"\n",
    "    to_fill_columns = list(set(gene_list) - set(X_df.columns))\n",
    "    print(\"mapping gene num:\" + str(len(gene_list) - len(to_fill_columns)))\n",
    "    padding_df = pd.DataFrame(np.zeros((X_df.shape[0], len(to_fill_columns))),\n",
    "                              columns=to_fill_columns,\n",
    "                              index=X_df.index)\n",
    "    X_df = pd.DataFrame(np.concatenate([df.values for df in [X_df, padding_df]], axis=1),\n",
    "                        index=X_df.index,\n",
    "                        columns=list(X_df.columns) + list(padding_df.columns))\n",
    "    X_df = X_df[gene_list]\n",
    "\n",
    "    var = pd.DataFrame(index=X_df.columns)\n",
    "    var['mask'] = [1 if i in to_fill_columns else 0 for i in list(var.index)]\n",
    "    return X_df, to_fill_columns, var\n",
    "\n",
    "\n",
    "class SCDataset(Dataset):\n",
    "    def __init__(self, adata, gene_list=None,label_to_int=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            args: object with attributes:\n",
    "                - data_path: path to the .h5ad file\n",
    "                - pre_normalized: 'T' or 'F', whether the data is already normalized\n",
    "            gene_list: full list of target genes (length >= 19264)\n",
    "            transform: optional transform to be applied on a sample\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.gene_list = gene_list\n",
    "        self.label_to_int = label_to_int\n",
    "        # Load and process data\n",
    "        # adata = sc.read_h5ad(data_path)\n",
    "        idx = adata.obs_names.tolist()\n",
    "        # _, label = np.unique(np.array(adata.obs[\"celltype\"]), return_inverse=True)\n",
    "        label = adata.obs[celltype_key].map(label_to_int).to_numpy()\n",
    "\n",
    "        try:\n",
    "            col = adata.var.gene_name.tolist()\n",
    "        except:\n",
    "            col = adata.var_names.tolist()\n",
    "\n",
    "        if issparse(adata.X):\n",
    "            gexpr = adata.X.toarray()\n",
    "        else:\n",
    "            gexpr = adata.X\n",
    "\n",
    "        gexpr = pd.DataFrame(gexpr, index=idx, columns=col)\n",
    "\n",
    "        if gexpr.shape[1] < 19264:\n",
    "            assert self.gene_list is not None, \"gene_list must be provided when gene count < 19264\"\n",
    "            gexpr, _, _ = main_gene_selection(gexpr, self.gene_list)\n",
    "            assert gexpr.shape[1] >= 19264\n",
    "\n",
    "        if args.pre_normalized == 'F':\n",
    "            print(\"preprocess data\")\n",
    "            adata = sc.AnnData(gexpr)\n",
    "            sc.pp.normalize_total(adata)\n",
    "            sc.pp.log1p(adata)\n",
    "            gexpr = pd.DataFrame(adata.X, index=adata.obs_names, columns=adata.var_names)\n",
    "\n",
    "        self.features = torch.tensor(gexpr.values, dtype=torch.float32)\n",
    "        self.label = label  # categorical string, optionally can map to int\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        expression = self.features[idx]\n",
    "        label = self.label[idx]\n",
    "        if self.transform:\n",
    "            expression = self.transform(expression)\n",
    "        return expression, label\n",
    "\n",
    "class Adapter(nn.Module):\n",
    "    def __init__(self, D_features=768, mlp_ratio=0.25, drop_rate=0.5, act_layer=nn.GELU, skip_connect=True):\n",
    "        super().__init__()\n",
    "        self.skip_connect = skip_connect\n",
    "        D_hidden_features = int(D_features * mlp_ratio)\n",
    "        self.act = act_layer()\n",
    "        self.D_fc1 = nn.Linear(D_features, D_hidden_features)\n",
    "        self.D_fc2 = nn.Linear(D_hidden_features, D_features)\n",
    "        self.dropout1 = nn.Dropout(drop_rate)\n",
    "        self.dropout2 = nn.Dropout(drop_rate)\n",
    "    def forward(self, x):\n",
    "        # x is (BT, HW+1, D)\n",
    "        xs = self.D_fc1(x)\n",
    "        xs = self.act(xs)\n",
    "        # dropout\n",
    "        xs = self.dropout1(xs)\n",
    "        xs = self.D_fc2(xs)\n",
    "        if self.skip_connect:\n",
    "            x = x + self.dropout2(xs)\n",
    "        else:\n",
    "            x = self.dropout2(xs)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LinearProbingClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, ckpt_path,prompt_settings,key,n_class,pool_type,frozenmore=True):\n",
    "        super().__init__()\n",
    "        self.ckpt_path = ckpt_path\n",
    "        self.frozenmore = frozenmore\n",
    "        self.key = key\n",
    "        self.prompt_settings = prompt_settings\n",
    "        self.n_class = n_class\n",
    "        self.pool_type = pool_type\n",
    "\n",
    "    def build(self):\n",
    "        model,model_config = load_model_frommmf(self.ckpt_path, self.prompt_settings, self.key)\n",
    "        self.token_emb = model.token_emb\n",
    "        self.pos_emb = model.pos_emb\n",
    "        self.encoder = model.encoder\n",
    "\n",
    "        if self.prompt_settings['prompt_type'] == 'head-prompt':\n",
    "            self.Space_Adapter = Adapter()\n",
    "\n",
    "        if self.pool_type == 'all':\n",
    "            self.fc1 = nn.Sequential(\n",
    "                nn.Linear(model_config['encoder']['hidden_dim']*4, 256),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(256, self.n_class)  # ['n_class']\n",
    "            )\n",
    "        elif self.pool_type == 'max':\n",
    "\n",
    "            self.fc1 = nn.Sequential(\n",
    "                nn.Linear(model_config['encoder']['hidden_dim'], model_config['encoder']['hidden_dim']),\n",
    "                nn.LayerNorm(model_config['encoder']['hidden_dim']),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(),\n",
    "\n",
    "                nn.Linear(model_config['encoder']['hidden_dim'], model_config['encoder']['hidden_dim']),\n",
    "                nn.LayerNorm(model_config['encoder']['hidden_dim']),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(),\n",
    "\n",
    "                nn.Linear(model_config['encoder']['hidden_dim'], self.n_class)  # ['n_class']\n",
    "            )\n",
    "        if self.pool_type == 'all':\n",
    "            self.norm = torch.nn.BatchNorm1d(model_config['encoder']['hidden_dim']*4, affine=False, eps=1e-6)\n",
    "            # self.norm = torch.nn.LayerNorm(model_config['encoder']['hidden_dim'] * 4)\n",
    "        elif self.pool_type == 'max':\n",
    "            self.norm = torch.nn.BatchNorm1d(model_config['encoder']['hidden_dim'], affine=False, eps=1e-6)\n",
    "            # self.norm = torch.nn.LayerNorm(model_config['encoder']['hidden_dim'])\n",
    "\n",
    "        self.model_config = model_config\n",
    "\n",
    "\n",
    "        keywords = ('lora', 'adapter', 'Adapter','prompt_embeddings')\n",
    "        for name, para in model.named_parameters():\n",
    "            para.requires_grad = False\n",
    "        params_to_update = filter(lambda p: any(keyword in p[0] for keyword in keywords),\n",
    "                                  model.named_parameters())\n",
    "        for _, param in params_to_update:\n",
    "            param.requires_grad = True\n",
    "        for na, param in self.fc1.named_parameters():\n",
    "            param.requires_grad = True\n",
    "        for na, param in self.norm.named_parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, data, *args, **kwargs):\n",
    "\n",
    "        x = data # (B, L)\n",
    "        # print(x.shape)\n",
    "        value_labels = x > 0\n",
    "\n",
    "\n",
    "\n",
    "        x, x_padding = gatherData(x, value_labels, self.model_config['pad_token_id'])\n",
    "        data_gene_ids = torch.arange(19264, device=x.device).repeat(x.shape[0], 1)\n",
    "        position_gene_ids, _ = gatherData(data_gene_ids, value_labels,\n",
    "                                        self.model_config['pad_token_id'])\n",
    "        # print(x.shape, flush=True)\n",
    "        x = self.token_emb(torch.unsqueeze(x, 2).float(), output_weight = 0)\n",
    "        position_emb = self.pos_emb(position_gene_ids)\n",
    "        x += position_emb\n",
    "\n",
    "        # print(x.shape, flush=True)\n",
    "        if self.prompt_settings['prompt_type'] == 'head-prompt':\n",
    "            x = self.Space_Adapter(x)\n",
    "\n",
    "        logits = self.encoder(x,x_padding)\n",
    "\n",
    "        # mlp\n",
    "        geneemb1 = logits[:, -1, :]\n",
    "        geneemb2 = logits[:, -2, :]\n",
    "        geneemb3, _ = torch.max(logits[:, :-2, :], dim=1)\n",
    "        geneemb4 = torch.mean(logits[:,:-2,:], dim=1)\n",
    "        if self.pool_type == 'all':\n",
    "            geneembmerge = torch.concat([geneemb1, geneemb2, geneemb3, geneemb4], axis=1)\n",
    "        elif self.pool_type == 'max':\n",
    "            geneembmerge, _ = torch.max(logits, dim=1)\n",
    "\n",
    "\n",
    "        logits = self.fc1(geneembmerge)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "class EarlyStopping():\n",
    "    def __init__(self, patience=20, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        elif self.best_loss - val_loss < self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\", flush=True)\n",
    "            if self.counter >= self.patience:\n",
    "                print('INFO: Early stopping', flush=True)\n",
    "                self.early_stop = True\n",
    "\n",
    "def compute_class_weights(sample_counts, cap=50):\n",
    "    max_count = max(sample_counts)\n",
    "    B = [max(max_count / c, cap) for c in sample_counts]\n",
    "    B = np.array(B)\n",
    "    weights = (B / B.sum())*B\n",
    "    return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def test(model: nn.Module, test_loader: DataLoader) -> float:\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    truths = []\n",
    "    for index, (data_t, labels_t) in enumerate(test_loader):\n",
    "        data_t, labels_t = data_t.to(device), labels_t.to(device)\n",
    "        logits = model(data_t)\n",
    "        softmax = nn.Softmax(dim=-1)\n",
    "        final_prob = softmax(logits)\n",
    "        final = final_prob.argmax(dim=-1)\n",
    "        final[np.amax(np.array(final_prob.detach().cpu().numpy()), axis=-1) < UNASSIGN_THRES] = -1\n",
    "        predictions.append(final.detach().cpu().numpy())\n",
    "        truths.append(labels_t.detach().cpu().numpy())\n",
    "        # tqdm.write(f'Batch {index + 1}/{len(test_loader)}')\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    truths = np.concatenate(truths, axis=0)\n",
    "\n",
    "    return predictions, truths\n",
    "\n"
   ],
   "id": "c798d36d15c5936a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)  # numpy random generator\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "gene_list_df = pd.read_csv('scFoundation/annotation/OS_scRNA_gene_index.19264.tsv', header=0, delimiter='\\t')\n",
    "gene_list = list(gene_list_df['gene_name'])\n",
    "\n",
    "# Load data\n",
    "# load_data(args)\n",
    "\n",
    "# Load model\n",
    "if args.version == 'noversion':\n",
    "    ckpt_path = args.model_path\n",
    "    key = None\n",
    "else:\n",
    "    ckpt_path = 'scFoundation/annotation/models/models.ckpt'\n",
    "    if args.output_type == 'cell':\n",
    "        if args.version == 'ce':\n",
    "            key = 'cell'\n",
    "        elif args.version == 'rde':\n",
    "            key = 'rde'\n",
    "        else:\n",
    "            raise ValueError('No version found')\n",
    "    elif args.output_type == 'gene':\n",
    "        key = 'gene'\n",
    "    elif args.output_type == 'gene_batch':\n",
    "        key = 'gene'\n",
    "    elif args.output_type == 'gene_expression':  # Not recommended\n",
    "        key = 'gene'\n",
    "    else:\n",
    "        raise ValueError('output_mode must be one of cell gene, gene_batch, gene_expression')\n",
    "print(key,flush=True)\n",
    "\n",
    "\n",
    "for fold_idx in range(0,5):\n",
    "    print('fold_idx:', fold_idx, flush=True)\n",
    "\n",
    "    train_datapath = f'{args.data_path}/{args.data_name}/{str(fold_idx)}/{args.data_name}_train{str(fold_idx)}.h5ad'\n",
    "    val_datapath = f'{args.data_path}/{args.data_name}/{str(fold_idx)}/{args.data_name}_val{str(fold_idx)}.h5ad'\n",
    "    test_datapath = f'{args.data_path}/{args.data_name}/{str(fold_idx)}/{args.data_name}_test{str(fold_idx)}.h5ad'\n",
    "\n",
    "    if args.data_name == 'ms':\n",
    "        celltype_key = 'celltype'\n",
    "    elif args.data_name == 'COVID':\n",
    "        celltype_key = 'cell_type'\n",
    "    elif args.data_name == 'NSCLC':\n",
    "        celltype_key = 'cell_type'\n",
    "\n",
    "    train_adata = sc.read_h5ad(train_datapath)\n",
    "    val_adata = sc.read_h5ad(val_datapath)\n",
    "    test_adata = sc.read_h5ad(test_datapath)\n",
    "\n",
    "    unique_labels = np.unique(train_adata.obs[celltype_key])\n",
    "    label_to_int = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "    train_dataset = SCDataset(train_adata, gene_list,label_to_int)\n",
    "    val_dataset = SCDataset(val_adata, gene_list,label_to_int)\n",
    "    test_dataset = SCDataset(test_adata, gene_list,label_to_int)\n",
    "\n",
    "    train_num = len(train_dataset)\n",
    "    train_class_num = np.unique(train_dataset.label, return_counts=True)[1]\n",
    "    n_class = len(np.unique(train_dataset.label))\n",
    "    print(f\"n_class: {n_class}\", flush=True)\n",
    "\n",
    "\n",
    "\n",
    "    sample_weights = 1.0 / train_class_num[train_dataset.label]\n",
    "    sample_weights = sample_weights / np.sum(sample_weights)\n",
    "    print(f\"sample_weights: {sample_weights}\", flush=True)\n",
    "    #\n",
    "\n",
    "    train_sampler = torch.utils.data.sampler.WeightedRandomSampler(sample_weights, train_num, replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, sampler=train_sampler)\n",
    "    # train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    print(len(train_loader), flush=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size)\n",
    "    print(len(val_loader), flush=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size)\n",
    "\n",
    "    model = LinearProbingClassifier(ckpt_path=ckpt_path,prompt_settings = prompt_settings,key= key,n_class=n_class,pool_type = args.pool_type)\n",
    "    model.build()\n",
    "    model = model.to(device)\n",
    "\n",
    "    print(\"-\" * 89, flush=True)\n",
    "    learnable_params = {k: v for k, v in model.named_parameters() if v.requires_grad == True}\n",
    "    for k, v in learnable_params.items():\n",
    "        print(f\"Learnable params {k} with shape {v.shape}\", flush=True)\n",
    "\n",
    "    print(\"-\" * 89, flush=True)\n",
    "\n",
    "\n",
    "    # loss_fn = nn.CrossEntropyLoss(weight=class_weight).to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.8, patience=5 ,min_lr=1e-7)  #0.9 10\n",
    "\n",
    "    print('start training...', flush=True)\n",
    "    bestmodel = None\n",
    "    max_acc = 0.0\n",
    "    best_val_loss = float(\"inf\")\n",
    "    early_stopping = EarlyStopping()\n",
    "    UNASSIGN_THRES = 0.0\n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "    # print(model, flush=True)\n",
    "    for i in range(args.epoch):\n",
    "        model.train()\n",
    "        # print(model, flush=True)\n",
    "        running_loss = 0.0\n",
    "        cum_acc = 0.0\n",
    "        predictions_train = []\n",
    "        truths_train = []\n",
    "        # print('start training...', flush=True)\n",
    "        for index, (data, labels) in enumerate(train_loader):\n",
    "            index += 1\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            logits = model(data)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            # print(loss.item())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), int(1e6))\n",
    "            optimizer.step()\n",
    "            # optimizer.zero_grad()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            final = softmax(logits)\n",
    "            final = final.argmax(dim=-1)\n",
    "            pred_num = labels.size(0)\n",
    "            truths_train.extend(labels)\n",
    "            predictions_train.extend(final)\n",
    "            correct_num = torch.eq(final, labels).sum(dim=-1)\n",
    "            cum_acc += torch.true_divide(correct_num, pred_num).mean().item()\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * cum_acc / len(train_loader)\n",
    "        print(f'    ==  Epoch: {i} | Training Loss: {epoch_loss:.6f} | Accuracy: {epoch_acc:6.4f}%  ==', flush=True)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        # print(model, flush=True)\n",
    "        running_loss = 0.0\n",
    "        predictions = []\n",
    "        truths = []\n",
    "        with torch.no_grad():\n",
    "            # print(len(val_loader), flush=True)\n",
    "            for index, (data_v, labels_v) in enumerate(val_loader):\n",
    "                index += 1\n",
    "                data_v, labels_v = data_v.to(device), labels_v.to(device)\n",
    "                logits = model(data_v)\n",
    "                val_loss = loss_fn(logits, labels_v)\n",
    "                running_loss += val_loss.item()\n",
    "                # softmax = nn.Softmax(dim=-1)\n",
    "                final_prob = softmax(logits)\n",
    "                final = final_prob.argmax(dim=-1)\n",
    "                # final[np.amax(np.array(final_prob.cpu()), axis=-1) < UNASSIGN_THRES] = -1\n",
    "                predictions.extend(final)\n",
    "\n",
    "                truths.extend(labels_v)\n",
    "\n",
    "            epoch_valloss = running_loss / len(val_loader)\n",
    "            scheduler.step(epoch_valloss)\n",
    "            del data_v, labels_v, logits, final_prob, final\n",
    "\n",
    "            predictions = torch.stack(predictions).cpu().numpy()\n",
    "            truths = torch.stack(truths).cpu().numpy()\n",
    "\n",
    "            # print(predictions.tolist())\n",
    "            # print(truths.tolist())\n",
    "\n",
    "            cur_acc = balanced_accuracy_score(truths, predictions)\n",
    "            f1 = f1_score(truths, predictions, average='macro')\n",
    "            accuracy = accuracy_score(truths, predictions)\n",
    "\n",
    "            print(\n",
    "                f'    ==  Epoch: {i} | Validation Loss: {epoch_valloss:.6f} | F1 Score: {f1:.6f} |  balanced accuracy: {cur_acc:.6f}  ==',\n",
    "                flush=True)\n",
    "\n",
    "            if epoch_valloss < best_val_loss:\n",
    "                best_val_loss = epoch_valloss\n",
    "                best_model = copy.deepcopy(model)\n",
    "                print(f\"Best model with loss {best_val_loss:5.4f}\", flush=True)\n",
    "            early_stopping(epoch_valloss)\n",
    "\n",
    "            print(classification_report(truths, predictions), flush=True)\n",
    "            # print(truths, flush=True)\n",
    "            # print(predictions, flush=True)\n",
    "            if early_stopping.early_stop:\n",
    "                break\n",
    "\n",
    "\n",
    "    torch.save(best_model.state_dict(),\n",
    "               ckpt_dir + args.data_name + '_' + args.peft_type + '_' + str(fold_idx)+ '_' + args.pre_normalized + '_'+ args.output_type + '_' + f\"best_model.pt\")\n",
    "    print('Best model saved successfully!', flush=True)\n",
    "    del predictions, truths\n",
    "\n",
    "\n",
    "\n",
    "    predictions, celltypes_labels = test(best_model, test_loader)\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score, balanced_accuracy_score,accuracy_score\n",
    "\n",
    "    accuracy = accuracy_score(celltypes_labels, predictions)\n",
    "    print(f\"fold {fold_idx} testing accuracy: {accuracy:.3f}\",flush=True)\n",
    "\n",
    "    balanced_accuracy = balanced_accuracy_score(celltypes_labels, predictions)\n",
    "    f1 = f1_score(celltypes_labels, predictions, average=\"macro\")\n",
    "    precision = precision_score(celltypes_labels, predictions, average=\"macro\")\n",
    "    recall = recall_score(celltypes_labels, predictions, average=\"macro\")\n",
    "\n",
    "    print(\n",
    "        f\"fold {fold_idx} testing macro Accuracy: {balanced_accuracy:.3f}, macro Precision: {precision:.3f},macro Recall: {recall:.3f}, \"f\"macro F1: {f1:.3f}\",\n",
    "        flush=True)\n",
    "    micro_f1 = f1_score(celltypes_labels, predictions, average=\"micro\")\n",
    "    micro_precision = precision_score(celltypes_labels, predictions, average=\"micro\")\n",
    "    micro_recall = recall_score(celltypes_labels, predictions, average=\"micro\")\n",
    "    print(\n",
    "        f\"fold {fold_idx} testing micro Accuracy: {balanced_accuracy:.3f}, micro Precision: {micro_precision:.3f},micro Recall: {micro_recall:.3f}, \"f\"micro F1: {micro_f1:.3f}\",\n",
    "        flush=True)\n"
   ],
   "id": "a3604828665ca61d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
